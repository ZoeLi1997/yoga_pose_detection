{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import os\n",
    "from random import randint\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# some cutoffs to play with:\n",
    "max_invalid_key_pairs = 8\n",
    "margin = 3\n",
    "singlefile = False\n",
    "filedir = \"\"\n",
    "filename = \"\"\n",
    "\n",
    "protoFile = \"pose/coco/pose_deploy_linevec.prototxt\"\n",
    "weightsFile = \"pose/coco/pose_iter_440000.caffemodel\"\n",
    "nPoints = 18\n",
    "# COCO Output Format\n",
    "keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', \n",
    "'R-Wr', 'L-Sho', 'L-Elb', 'L-Wr', \n",
    "'R-Hip', 'R-Knee', 'R-Ank', 'L-Hip',\n",
    " 'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', \n",
    " 'R-Ear', 'L-Ear']\n",
    "\n",
    "POSE_PAIRS = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7],\n",
    "              [1,8], [8,9], [9,10], [1,11], [11,12], [12,13],\n",
    "              [1,0], [0,14], [14,16], [0,15], [15,17],\n",
    "              [2,17], [5,16] ]\n",
    "\n",
    "# index of pafs correspoding to the POSE_PAIRS\n",
    "# e.g for POSE_PAIR(1,2), the PAFs are located at indices (31,32) of output, Similarly, (1,5) -> (39,40) and so on.\n",
    "mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44],\n",
    "          [19,20], [21,22], [23,24], [25,26], [27,28], [29,30],\n",
    "          [47,48], [49,50], [53,54], [51,52], [55,56],\n",
    "          [37,38], [45,46]]\n",
    "\n",
    "colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],\n",
    "         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],\n",
    "         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeypoints(probMap, threshold=0.1):\n",
    "\n",
    "    mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)\n",
    "\n",
    "    mapMask = np.uint8(mapSmooth>threshold)\n",
    "    keypoints = []\n",
    "\n",
    "    #find the blobs\n",
    "    contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    #for each blob find the maxima\n",
    "    for cnt in contours:\n",
    "        blobMask = np.zeros(mapMask.shape)\n",
    "        blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)\n",
    "        maskedProbMap = mapSmooth * blobMask\n",
    "        _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)\n",
    "        keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],))\n",
    "\n",
    "    return keypoints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find valid connections between the different joints of a all persons present\n",
    "def getValidPairs(output, frameWidth, frameHeight, detected_keypoints):\n",
    "    valid_pairs = []\n",
    "    invalid_pairs = []\n",
    "    n_interp_samples = 10\n",
    "    paf_score_th = 0.1\n",
    "    conf_th = 0.7\n",
    "    # loop for every POSE_PAIR\n",
    "    for k in range(len(mapIdx)):\n",
    "        # A->B constitute a limb\n",
    "        pafA = output[0, mapIdx[k][0], :, :]\n",
    "        pafB = output[0, mapIdx[k][1], :, :]\n",
    "        pafA = cv2.resize(pafA, (frameWidth, frameHeight))\n",
    "        pafB = cv2.resize(pafB, (frameWidth, frameHeight))\n",
    "\n",
    "        # Find the keypoints for the first and second limb\n",
    "        candA = detected_keypoints[POSE_PAIRS[k][0]]\n",
    "        candB = detected_keypoints[POSE_PAIRS[k][1]]\n",
    "        nA = len(candA)\n",
    "        nB = len(candB)\n",
    "\n",
    "        # If keypoints for the joint-pair is detected\n",
    "        # check every joint in candA with every joint in candB\n",
    "        # Calculate the distance vector between the two joints\n",
    "        # Find the PAF values at a set of interpolated points between the joints\n",
    "        # Use the above formula to compute a score to mark the connection valid\n",
    "\n",
    "        if( nA != 0 and nB != 0):\n",
    "            valid_pair = np.zeros((0,3))\n",
    "            for i in range(nA):\n",
    "                max_j=-1\n",
    "                maxScore = -1\n",
    "                found = 0\n",
    "                for j in range(nB):\n",
    "                    # Find d_ij\n",
    "                    d_ij = np.subtract(candB[j][:2], candA[i][:2])\n",
    "                    norm = np.linalg.norm(d_ij)\n",
    "                    if norm:\n",
    "                        d_ij = d_ij / norm\n",
    "                    else:\n",
    "                        continue\n",
    "                    # Find p(u)\n",
    "                    interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),\n",
    "                                            np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))\n",
    "                    # Find L(p(u))\n",
    "                    paf_interp = []\n",
    "                    for k in range(len(interp_coord)):\n",
    "                        paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],\n",
    "                                           pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ])\n",
    "                    # Find E\n",
    "                    paf_scores = np.dot(paf_interp, d_ij)\n",
    "                    avg_paf_score = sum(paf_scores)/len(paf_scores)\n",
    "\n",
    "                    # Check if the connection is valid\n",
    "                    # If the fraction of interpolated vectors aligned with PAF is higher then threshold -> Valid Pair\n",
    "                    if ( len(np.where(paf_scores > paf_score_th)[0]) / n_interp_samples ) > conf_th :\n",
    "                        if avg_paf_score > maxScore:\n",
    "                            max_j = j\n",
    "                            maxScore = avg_paf_score\n",
    "                            found = 1\n",
    "                # Append the connection to the list\n",
    "                if found:\n",
    "                    valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0)\n",
    "\n",
    "            # Append the detected connections to the global list\n",
    "            valid_pairs.append(valid_pair)\n",
    "        else: # If no keypoints are detected\n",
    "            #print(\"No Connection : k = {}\".format(k))\n",
    "            invalid_pairs.append(k)\n",
    "            valid_pairs.append([])\n",
    "    return valid_pairs, invalid_pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function creates a list of keypoints belonging to each person\n",
    "# For each detected valid pair, it assigns the joint(s) to a person\n",
    "def getPersonwiseKeypoints(valid_pairs, invalid_pairs, keypoints_list):\n",
    "    # the last number in each row is the overall score\n",
    "    personwiseKeypoints = -1 * np.ones((0, 19))\n",
    "\n",
    "    for k in range(len(mapIdx)):\n",
    "        if k not in invalid_pairs:\n",
    "            partAs = valid_pairs[k][:,0]\n",
    "            partBs = valid_pairs[k][:,1]\n",
    "            indexA, indexB = np.array(POSE_PAIRS[k])\n",
    "\n",
    "            for i in range(len(valid_pairs[k])):\n",
    "                found = 0\n",
    "                person_idx = -1\n",
    "                for j in range(len(personwiseKeypoints)):\n",
    "                    if personwiseKeypoints[j][indexA] == partAs[i]:\n",
    "                        person_idx = j\n",
    "                        found = 1\n",
    "                        break\n",
    "\n",
    "                if found:\n",
    "                    personwiseKeypoints[person_idx][indexB] = partBs[i]\n",
    "                    personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2]\n",
    "\n",
    "                # if find no partA in the subset, create a new subset\n",
    "                elif not found and k < 17:\n",
    "                    row = -1 * np.ones(19)\n",
    "                    row[indexA] = partAs[i]\n",
    "                    row[indexB] = partBs[i]\n",
    "                    # add the keypoint_scores for the two keypoints and the paf_score\n",
    "                    row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2]\n",
    "                    personwiseKeypoints = np.vstack([personwiseKeypoints, row])\n",
    "    return personwiseKeypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def perform_keypoint_analysis(filename):\n",
    "    print(filename)\n",
    "    image1 = cv2.imread(filename)\n",
    "    \n",
    "    frameWidth = image1.shape[1]\n",
    "    frameHeight = image1.shape[0]\n",
    "\n",
    "    t = time.time()\n",
    "    net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "    # Fix the input Height and get the width according to the Aspect Ratio\n",
    "    inHeight = 368\n",
    "    inWidth = int((inHeight/frameHeight)*frameWidth)\n",
    "\n",
    "    inpBlob = cv2.dnn.blobFromImage(image1, 1.0 / 255, (inWidth, inHeight),\n",
    "                            (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "    net.setInput(inpBlob)\n",
    "    output = net.forward()\n",
    "    #print(\"Time Taken in forward pass = {}\".format(time.time() - t))\n",
    "\n",
    "    detected_keypoints = []\n",
    "    keypoints_list = np.zeros((0,3))\n",
    "    keypoint_id = 0\n",
    "    threshold = 0.1\n",
    "\n",
    "    for part in range(nPoints):\n",
    "        probMap = output[0,part,:,:]\n",
    "        probMap = cv2.resize(probMap, (image1.shape[1], image1.shape[0]))\n",
    "        keypoints = getKeypoints(probMap, threshold)\n",
    "        #print(\"Keypoints - {} : {}\".format(keypointsMapping[part], keypoints))\n",
    "        keypoints_with_id = []\n",
    "        for i in range(len(keypoints)):\n",
    "            keypoints_with_id.append(keypoints[i] + (keypoint_id,))\n",
    "            keypoints_list = np.vstack([keypoints_list, keypoints[i]])\n",
    "            keypoint_id += 1\n",
    "\n",
    "        detected_keypoints.append(keypoints_with_id)\n",
    "\n",
    "\n",
    "    frameClone = image1.copy()\n",
    "    for i in range(nPoints):\n",
    "        for j in range(len(detected_keypoints[i])):\n",
    "            cv2.circle(frameClone, detected_keypoints[i][j][0:2], 5, colors[i], -1, cv2.LINE_AA)\n",
    "    #cv2.imshow(\"Keypoints\",frameClone)\n",
    "\n",
    "    valid_pairs, invalid_pairs = getValidPairs(output, frameWidth, frameHeight, detected_keypoints)\n",
    "\n",
    "    personwiseKeypoints = getPersonwiseKeypoints(valid_pairs, invalid_pairs, keypoints_list)\n",
    "\n",
    "    for i in range(17):\n",
    "        for n in range(len(personwiseKeypoints)):\n",
    "            index = personwiseKeypoints[n][np.array(POSE_PAIRS[i])]\n",
    "            if -1 in index:\n",
    "                continue\n",
    "            B = np.int32(keypoints_list[index.astype(int), 0])\n",
    "            A = np.int32(keypoints_list[index.astype(int), 1])\n",
    "            cv2.line(frameClone, (B[0], A[0]), (B[1], A[1]), colors[i], 3, cv2.LINE_AA)\n",
    "    if(len(invalid_pairs) > max_invalid_key_pairs):\n",
    "        savedname = \"thrown_out/\" + filename\n",
    "        cv2.imwrite(savedname , frameClone)\n",
    "    elif(len(invalid_pairs) > max_invalid_key_pairs + margin):\n",
    "        savedname = \"on_the_edge/\" + filename\n",
    "        cv2.imwrite(savedname , frameClone)\n",
    "    else:\n",
    "        savedname = \"approved/\" + filename\n",
    "        cv2.imwrite(savedname , frameClone)\n",
    "\n",
    "\n",
    "\n",
    "    return valid_pairs, invalid_pairs\n",
    "    #cv2.waitKey(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_file(filename):\n",
    "    valid, invalid = perform_keypoint_analysis(filename)\n",
    "    missing = []\n",
    "    print(\"File: \" + filename)\n",
    "    i = 0\n",
    "    for el in valid:\n",
    "        if el != []:\n",
    "            missing.append(i)\n",
    "        i += 1\n",
    "    print(\"number of valid pairs: \" + str(len(missing)))\n",
    "    print(missing)\n",
    "    for el in missing:\n",
    "        tupl = POSE_PAIRS[el]\n",
    "        print(str(keypointsMapping[tupl[0]]) + \" -> \" + str(keypointsMapping[tupl[1]]))\n",
    "\n",
    "    print(\"number of invalid pairs: \" + str(len(invalid)))\n",
    "    print(invalid)\n",
    "    \n",
    "    for el in invalid:\n",
    "        tupl = POSE_PAIRS[el]\n",
    "        print(str(keypointsMapping[tupl[0]]) + \" -> \" + str(keypointsMapping[tupl[1]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns true if passed filter, false if not\n",
    "def apply_filter(invalid):\n",
    "    '''\n",
    "        keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', \n",
    "        'R-Wr', 'L-Sho', 'L-Elb', 'L-Wr', \n",
    "        'R-Hip', 'R-Knee', 'R-Ank', 'L-Hip',\n",
    "         'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', \n",
    "         'R-Ear', 'L-Ear']\n",
    "\n",
    "        POSE_PAIRS = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7],\n",
    "                      [1,8], [8,9], [9,10], [1,11], [11,12], [12,13],\n",
    "                      [1,0], [0,14], [14,16], [0,15], [15,17],\n",
    "                      [2,17], [5,16] ]\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    for each list, you need at least one keypoint:\n",
    "    [r-eye, l-eye, 'r-ear', 'l-ear']\n",
    "    [r-elb, r-wr]\n",
    "    [l-elb, l-wr]\n",
    "    \n",
    "    need all:\n",
    "    [l-ank, r-ank]\n",
    "    '''\n",
    "    # set up lists of keypoints that must not be in the invalid list\n",
    "    required_keypoints = []\n",
    "    minimum_keypoints = []\n",
    "    for item in [\"L-Ank\", \"R-Ank\"]:\n",
    "        required_keypoints.append(keypointsMapping.index(item))\n",
    "\n",
    "    for outer in [['R-Eye', 'L-Eye', 'R-Ear', 'L-Ear'], \n",
    "                 ['R-Elb', 'R-Wr'],\n",
    "                 ['L-Elb', 'L-Wr']\n",
    "                ]:\n",
    "        to_append = []\n",
    "        for inner in outer:\n",
    "            to_append.append(keypointsMapping.index(inner))\n",
    "        minimum_keypoints.append(to_append)\n",
    "        \n",
    "    print(required_keypoints)\n",
    "    print(minimum_keypoints)\n",
    "    \n",
    "    # now check if the required ones are all there\n",
    "    for req_kp in required_keypoints:\n",
    "        if req_kp in invalid:\n",
    "            return False\n",
    "    \n",
    "    # now check that at least one of each list for minimum_keypoints is not in invalid\n",
    "    for kp_set in minimum_keypoints:\n",
    "        kp_valid = [i for i in kp_set if i not in invalid]\n",
    "        if len(kp_valid) < 1:\n",
    "            return False\n",
    "    return True\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_folder(filedir):\n",
    "    if os.path.isdir(filedir) is False:\n",
    "        print(filedir + \" is not a folder.\")\n",
    "        return None\n",
    "    elif filedir[len(filedir) - 1] is not \"/\":\n",
    "        filedir += \"/\"\n",
    "    try:\n",
    "        os.mkdir(\"approved\")\n",
    "    except OSError:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir(\"on_the_edge\")\n",
    "    except OSError:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir(\"thrown_out\")\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(\"approved/\" + filedir)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(\"on_the_edge/\" + filedir)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(\"thrown_out/\" + filedir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    onlyfiles = [filedir + f for f in listdir(filedir) if (isfile(join(filedir, f)) and (\".jpg\" in f or \".png\" in f))]\n",
    "    print(onlyfiles)\n",
    "    good_files = []\n",
    "    bad_files = []\n",
    "    iffy_files = []\n",
    "    onlyfiles = [onlyfiles[0]]\n",
    "    \n",
    "    for filename in onlyfiles:\n",
    "        valid, invalid = perform_keypoint_analysis(filename)\n",
    "        if len(invalid) > max_invalid_key_pairs:\n",
    "            #print(\"throwing out image: \" + filename + \" due to high number of invalid pairs\")\n",
    "            bad_files.append(filename)\n",
    "        elif len(invalid) > (max_invalid_key_pairs + margin):\n",
    "            iffy_files.append(filename)\n",
    "        else:\n",
    "            good_files.append(filename)\n",
    "\n",
    "        #print(\"~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "\n",
    "    print(\"NUM ACCEPTED BY FILTER IN DIR \" + filedir + \": \" + str(len(good_files)) + \"\\nNOT ACCEPTED: \" + str(len(bad_files)))\n",
    "    print(\"accepted: \\n\")\n",
    "    print(good_files)\n",
    "    print(\"\\n\\nnot accepted:\")\n",
    "    print(bad_files)\n",
    "\n",
    "    print(\"manually check iffy files:\")\n",
    "    print(iffy_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warrior2/File12.jpg\n",
      "File: warrior2/File12.jpg\n",
      "number of valid pairs: 15\n",
      "[0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17]\n",
      "Neck -> R-Sho\n",
      "Neck -> L-Sho\n",
      "L-Sho -> L-Elb\n",
      "L-Elb -> L-Wr\n",
      "Neck -> R-Hip\n",
      "R-Hip -> R-Knee\n",
      "R-Knee -> R-Ank\n",
      "Neck -> L-Hip\n",
      "L-Hip -> L-Knee\n",
      "L-Knee -> L-Ank\n",
      "Neck -> Nose\n",
      "Nose -> R-Eye\n",
      "Nose -> L-Eye\n",
      "L-Eye -> L-Ear\n",
      "R-Sho -> L-Ear\n",
      "number of invalid pairs: 4\n",
      "[2, 3, 14, 18]\n",
      "R-Sho -> R-Elb\n",
      "R-Elb -> R-Wr\n",
      "R-Eye -> R-Ear\n",
      "L-Sho -> R-Ear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "classify_file(\"warrior2/File12.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['warrior2/File46.jpg', 'warrior2/File47.jpg', 'warrior2/File53.png', 'warrior2/File51.jpg', 'warrior2/File45.jpg', 'warrior2/File79.jpg', 'warrior2/File78.jpg', 'warrior2/File44.jpg', 'warrior2/File50.jpg', 'warrior2/File68.jpg', 'warrior2/File54.jpg', 'warrior2/File55.jpg', 'warrior2/File69.jpg', 'warrior2/File43.jpg', 'warrior2/File57.jpg', 'warrior2/File80.jpg', 'warrior2/File56.jpg', 'warrior2/File42.png', 'warrior2/File81.png', 'warrior2/File25.jpg', 'warrior2/File19.jpg', 'warrior2/File18.jpg', 'warrior2/File30.jpg', 'warrior2/File24.jpg', 'warrior2/File32.jpg', 'warrior2/File26.jpg', 'warrior2/File27.jpg', 'warrior2/File33.jpg', 'warrior2/File37.jpg', 'warrior2/File20.jpg', 'warrior2/File35.jpg', 'warrior2/File21.jpg', 'warrior2/File38.jpg', 'warrior2/File39.jpg', 'warrior2/File13.jpg', 'warrior2/File12.jpg', 'warrior2/File16.jpg', 'warrior2/File17.jpg', 'warrior2/File15.jpg', 'warrior2/File14.jpg', 'warrior2/File67.jpg', 'warrior2/File73.jpg', 'warrior2/File66.jpg', 'warrior2/File72.png', 'warrior2/File70.jpg', 'warrior2/File64.jpg', 'warrior2/File58.jpg', 'warrior2/File59.jpg', 'warrior2/File49.jpg', 'warrior2/File75.jpg', 'warrior2/File61.jpg', 'warrior2/File60.jpg', 'warrior2/File74.jpg', 'warrior2/File48.jpg', 'warrior2/File62.jpg', 'warrior2/File76.jpg', 'warrior2/File77.jpg']\n",
      "warrior2/File46.jpg\n",
      "NUM ACCEPTED BY FILTER IN DIR warrior2/: 1\n",
      "NOT ACCEPTED: 0\n",
      "accepted: \n",
      "\n",
      "['warrior2/File46.jpg']\n",
      "\n",
      "\n",
      "not accepted:\n",
      "[]\n",
      "manually check iffy files:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "classify_folder(\"warrior2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
